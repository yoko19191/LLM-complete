{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零预训练LLAMA3的完整指南：一个文件，探索Scaling Law\n",
    "\n",
    "原文地址: https://zhuanlan.zhihu.com/p/706097271?utm_campaign=&utm_medium=social&utm_psn=1790423752451944448\n",
    "\n",
    "作者受到Andrew从零开始预训练GPT-2的四小时视频的启发，打算复刻 LlaMA3 从零开始预训练一个大预言模型。\n",
    "\n",
    "代码开源在这里：\n",
    "https://github.com/hengjiUSTC/learn-llm/tree/main/pretrain\n",
    "\n",
    "内容包含：\n",
    "\n",
    "1. 模型评估\n",
    "2. 模型构建(RMSNorm, RotaryEmbedding, Attention, MLP, Block 组装)   \n",
    "3. 使用 Distributed Data Parallel 策略从零开始预训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又来了一个\n",
    "\n",
    "[Llama3模型,从零构件复现,使用RLHF方法训练.代码实战.](https://www.bilibili.com/video/BV1mZ421M7Wm/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=427a8f6991c46f06262700ed0e9203dc)\n",
    "\n",
    "https://github.com/lansinuote/Simple_RLHF_Llama3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.14\n",
      "device: mps\n",
      "None\n",
      "None\n",
      "Disk Device: /dev/disk3s3s1, Total Size: 460.43 GB\n",
      "Disk Device: /dev/disk3s6, Total Size: 460.43 GB\n",
      "Disk Device: /dev/disk3s4, Total Size: 460.43 GB\n",
      "Disk Device: /dev/disk3s2, Total Size: 460.43 GB\n",
      "Disk Device: /dev/disk1s2, Total Size: 0.49 GB\n",
      "Disk Device: /dev/disk1s1, Total Size: 0.49 GB\n",
      "Disk Device: /dev/disk1s3, Total Size: 0.49 GB\n",
      "Disk Device: /dev/disk3s1, Total Size: 460.43 GB\n",
      "Disk Device: /Applications/微信读书.app/Wrapper, Total Size: 460.43 GB\n",
      "Disk Device: /Applications/小宇宙.app/Wrapper, Total Size: 460.43 GB\n",
      "Disk Device: /dev/disk2s1, Total Size: 5.00 GB\n",
      "Disk Device: /dev/disk3s3, Total Size: 460.43 GB\n",
      "Disk Device: /Applications/微信读书.app/Wrapper, Total Size: 460.43 GB\n",
      "CUDA not available\n",
      "Current directory: /Users/guchen/repo/LLM-complete/projects/llama3-from-scratch\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# Existing code\n",
    "print(platform.python_version())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "print(torch.version.cuda if torch.version.cuda else \"None\")\n",
    "print(torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else \"None\")\n",
    "\n",
    "\n",
    "# For Linux systems, read the /proc/cpuinfo file for CPU details\n",
    "# if os.name == 'posix':\n",
    "#     with open('/proc/cpuinfo') as f:\n",
    "#         cpuinfo = f.read()\n",
    "#     print(cpuinfo)  # This will print the content of the cpuinfo which includes detailed CPU information\n",
    "\n",
    "\n",
    "# Get disk capacity\n",
    "disk_partitions = psutil.disk_partitions()\n",
    "for partition in disk_partitions:\n",
    "    partition_usage = psutil.disk_usage(partition.mountpoint)\n",
    "    print(f\"Disk Device: {partition.device}, Total Size: {partition_usage.total / (1024 * 1024 * 1024):.2f} GB\")\n",
    "\n",
    "# Get CUDA version, similar to the existing code for when CUDA is not available\n",
    "print(torch.version.cuda if torch.cuda.is_available() else \"CUDA not available\")\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory: {current_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Meta Llama3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) + [hellaswag 评估](https://paperswithcode.com/sota/sentence-completion-on-hellaswag)\n",
    "\n",
    "Llama3-8B 用了 15 trillions tokens with 10M human annotated examples. knowledge cutoff at March, 2023.\n",
    "\n",
    "在 Meta's Research SuperCluster 话费了 1.3M GPU Hours(H100 run 148yr), 相对应的 70B 是 6.4M GPU Hours.\n",
    "\n",
    "官方的 Llama3-8B 的 hellaswag accuracy 性能应该在 70 左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' \n",
    "\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_cache_dir = \"/mnt/workspace/models \"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=model_path,\n",
    "    local_dir=model_cache_dir,\n",
    "    #proxies={\"https\": \"http://localhost:7890\"},\n",
    "    max_workers=8,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "多头使用了 Grouped-Query Attention (GQA) 来提高推理速度\n",
    "\n",
    "\n",
    "位置编码使用 RoPE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_ground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
