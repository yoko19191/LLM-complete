{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LangChain快速入门 (学习笔记)\n",
    "\n",
    "https://techdiylife.github.io/blog/topic.html?category2=t07&blogid=0043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一个大型语言模型，由 Google 训练。\\n\\n我可以生成文本、翻译语言、编写不同类型的创意内容，并尝试回答您的问题以尽我所能提供有用的信息。\\n\\n我仍在学习中，但我的目标是成为一个有用的工具，可以帮助人们理解世界并完成任务。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma2\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "llm.invoke(\"你是谁？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"name\": \"小王\",\\n  \"role\": \"AI助手\"\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = Ollama(model=\"qwen2\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"现在你的名字叫小王.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke(\"你是谁？ 用JSON格式输出\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文中没有提到服务型制造研究院所在的具体城市，因此无法回答这个问题。\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 定义提示模板\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 初始化 StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 创建链\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 调用链并获取结果\n",
    "result = chain.invoke({\"context\": \"服务型制造研究院在一个科研机构\", \"question\": \"服务型制造研究院在地处什么城市?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索前：自由主义女权（Liberal feminism）是女权运动的一个重要流派，它强调通过法律、政治和经济上的改革来实现性别平等。自由主义女权的核心理念在于，女性应该享有与男性相同的权利，包括受教育的权利、就业的机会、工资的公平性以及参与民主决策的过程。这个理论认为，通过改变制度和社会结构中的不平等待遇，可以逐步消除性别歧视。\n",
      "\n",
      "自由主义女权主张：\n",
      "\n",
      "1. **法律平等**：通过立法来确保男女在法律面前平等对待，比如废除针对女性的歧视性法律、推动男女同工同酬等。\n",
      "2. **经济独立**：强调女性应该有获得良好教育和职业发展的机会，以提高其经济自立能力。\n",
      "3. **政治参与**：鼓励女性参与政治和社会事务，增加她们在决策过程中的代表性和影响力。\n",
      "4. **消除性别角色刻板印象**：倡导打破社会对男女角色的传统期望，促进性别平等的社会观念。\n",
      "\n",
      "自由主义女权理论与激进女权、社会主义女权等其他女权派别有所不同。它更侧重于通过渐进式的变革来实现目标，并认为在现行的法律和制度框架内进行改革是可行且有效的途径。\n",
      "检索前（运行测试）：自由主义女权（feminism-liberal）指的是基于自由主义思想的女权观点或运动。它强调个人自由、平等以及通过法律和制度来实现性别平等等核心原则。\n",
      "\n",
      "自由主义女权通常主张以下几点：\n",
      "\n",
      "1. 法律面前人人平等：认为男女应当在法律面前享有同等的权利，包括但不限于工作权利、教育机会、投票权和平等的薪资待遇。\n",
      "2. 家庭与职场的自主选择：支持个体根据自己的意愿和能力在家庭生活和社会活动中做出自由决定。这可能涉及到反对传统的性别角色分配，并提倡父亲和母亲都有平等参与育儿的责任。\n",
      "3. 政策改革以促进平等：倡导政策上的改变，如平等薪资、反性骚扰法、工作与生活平衡的措施等，以减少性别歧视并确保女性在社会各个领域享有公平的机会。\n",
      "4. 个人选择权和自主权：强调个体对于自己的身体和生活方式有决策权，并支持合法的人权和自由权利，包括但不限于堕胎权、同性婚姻以及性别认同的权利。\n",
      "\n",
      "总之，自由主义女权关注通过法律、政策和社会改革来促进男女之间的平等和保护个人的自由。它不依赖于特定的社会或文化环境中的传统角色分配，而是强调个体的自我实现和个人自由。\n",
      "检索后：自由主义女权是指一种政治哲学观点，它将女性的权益和地位置于个人自由和民主制度的核心。在这一框架下，女权主义者强调法律平等待遇、性别平等以及对基于性别的歧视的反对。他们主张通过促进公平和正义来实现这些目标，包括提供教育和职业机会的平等准入、禁止性别薪酬差距、以及支持女性权利的立法。\n",
      "\n",
      "自由主义女权主义者通常会倡导一些核心原则，比如个人选择的权利（例如关于堕胎、同性婚姻等）、自我决定的能力和不受暴力侵害的权利。他们可能也会批判传统家庭角色中对女性的期望，并寻求创造一种更加平等的家庭结构，在这种结构中，家务和育儿责任由男性和女性共同承担。\n",
      "\n",
      "尽管自由主义女权主义者通常支持政府采取措施来促进性别平等（例如通过立法或社会政策），但他们在具体措施上可能会有分歧。一些人可能更倾向于通过法律强制执行性别共享的家务分工，而其他人则可能认为这种干预会过多地限制个人选择和家庭结构的可能性。\n",
      "\n",
      "总的来说，自由主义女权强调个体权利、法治和民主参与，并主张政府在确保平等和自由方面发挥作用。\n"
     ]
    }
   ],
   "source": [
    "# Retrieval Chain 检索链条\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install faiss-cpu\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 第一步：初始化模型对象：LLM，Embedding\n",
    "llm = Ollama(model=\"qwen2\")\n",
    "embeddings = OllamaEmbeddings(model=\"qwen2\")\n",
    "response1 = llm.invoke(\"自由主义女权是什么？？\")\n",
    "print(f\"检索前：{response1}\")\n",
    "\n",
    "\n",
    "# 第二步：获取数据\n",
    "loader = WebBaseLoader(\"https://plato.stanford.edu/entries/feminism-liberal/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 第三步：文本拆分，文本Vector化\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "\n",
    "# 第四步：准备Prompt，创建文档处理的LLMChain，检索Chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain) #建立检索\n",
    "\n",
    "# 第五步：代码执行与测试\n",
    "response2 = document_chain.invoke({\n",
    "    \"input\": \"自由主义女权是什么？\",\n",
    "    \"context\": [Document(page_content=\"feminism-liberal\")]\n",
    "})\n",
    "print(f\"检索前（运行测试）：{response2}\")\n",
    "\n",
    "response3 = retrieval_chain.invoke({\"input\": \"自由主义女权是什么？\"})\n",
    "print(f\"检索后：{response3['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在使用LangSmith进行LLM应用的测试时，可以遵循以下步骤来优化和监控你的应用程序：\n",
      "\n",
      "### 1. **快速启动（Quick Start）**\n",
      "- 访问LangSmith并注册一个帐户。\n",
      "- 启用追踪功能以便跟踪应用程序的性能。\n",
      "\n",
      "### 2. **用户指南（User Guide）**\n",
      "- 阅读用户指南以了解如何使用平台的各项功能，包括测试、评估和生产监控等。\n",
      "\n",
      "### 3. **Beta 测试（Beta Testing）**\n",
      "- 在开发初期或Beta阶段启用beta测试模式。这能让你收集应用在真实场景下的数据，并观察其表现。\n",
      "- 进行用户反馈收集，分析应用的性能并标注问题点。\n",
      "\n",
      "### 4. **捕获反馈（Capturing Feedback）**\n",
      "- 启动应用时，确保从用户那里获取有关生成响应的人类反馈。这有助于识别有趣和边缘情况的问题。\n",
      "- 在LangSmith中为反馈设置评分，并使用特定的反馈标记过滤追踪，以便专注于低分反馈。\n",
      "\n",
      "### 5. **标注追踪（Annotating Traces）**\n",
      "- 使用注释功能对应用性能进行详细说明。这可以帮助你捕获关键数据点、构建基准测试集以及在需要时进行人工标注。\n",
      "- 安排内部或外部专家作为注释者，以帮助标记不同的应用版本并比较它们的性能。\n",
      "\n",
      "### 6. **添加到数据集（Adding Runs to a Dataset）**\n",
      "- 随着应用程序进入Beta阶段和生产阶段，请继续收集数据以优化其性能。将运行结果添加到数据集中，以便于在实际场景下进行广泛的测试覆盖。\n",
      "- 利用LangSmith的自动评分、注释队列和数据集功能来处理大规模数据并进行比较。\n",
      "\n",
      "### 7. **生产监控与自动化（Production Monitoring & Automations）**\n",
      "- 监控应用程序性能的关键指标（如延迟、成本和反馈分数），确保在大量用户中提供理想的结果。\n",
      "- 集成实时评估和自动化流程，用于在生产环境中快速处理和评分追踪。\n",
      "- 利用线程功能来整理单一对话中的追踪，以便跟踪应用的多轮交互。\n",
      "\n",
      "### 8. **深入调试（Debugging）**\n",
      "- 使用LangSmith提供的调试信息，分析和识别序列中可能出现的问题点。这可以帮助你理解模型的运行方式并快速解决问题。\n",
      "\n",
      "通过遵循这些步骤和利用LangSmith的特性，你可以有效测试、优化和监控你的LLM应用程序，并确保其在各种场景下都能提供高质量的表现。\n"
     ]
    }
   ],
   "source": [
    "#  基于历史对话记录的检索Chain\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 第一步： 准备LLM，Embedding模型，读取数据，数据处理\n",
    "llm = Ollama(model=\"qwen2\")\n",
    "llm.invoke(\"langsmith是做什么的？\")\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"qwen2\") #模型默认为llama2\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# 第二步：创建检索Chain，通过LLM根据历史对话记录检索相关内容\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"根据上述对话，生成一个搜索查询以查找与对话相关的信息\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "# 动作确认：用来确认retriever_chain的执行情况，执行检索，不会真的回答问题\"告诉我怎么做\"\n",
    "#chat_history = [HumanMessage(content=\"LangSmith的网址？\"), AIMessage(content=\"可以！\")]\n",
    "#response1 = retriever_chain.invoke({\n",
    "#    \"chat_history\": chat_history,\n",
    "#    \"input\": \"告诉我怎么做\"\n",
    "#})\n",
    "\n",
    "# 第三步：创建检索chain，根据检索出的内容，历史对话，与用户的问题产生回答\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"根据下面的上下文回答用户的问题：\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "# 第四步：代码执行与测试\n",
    "chat_history = [HumanMessage(content=\"LangSmith能帮助测试我的LLM应用吗？\"), AIMessage(content=\"可以！\")]\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"告诉我怎么做\"\n",
    "})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'version_short' from 'pydantic.version' (d:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\pydantic\\version.cp311-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, AIMessage\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retriever_tool\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hub\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_openai_functions_agent\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py:22\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m itemgetter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     overload,\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageModelInput\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangSmithParams\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\openai\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_os\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\openai\\types\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Batch \u001b[38;5;28;01mas\u001b[39;00m Batch\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model \u001b[38;5;28;01mas\u001b[39;00m Model\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\openai\\types\\batch.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_error\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_request_counts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchRequestCounts\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\openai\\_models.py:21\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Unpack,\n\u001b[0;32m      9\u001b[0m     Literal,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     runtime_checkable,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerics\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     Body,\n\u001b[0;32m     26\u001b[0m     IncEx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     HttpxRequestFiles,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\pydantic\\generics.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The `generics` module is a backport module from V1.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[0;32m      4\u001b[0m \u001b[38;5;21m__getattr__\u001b[39m \u001b[38;5;241m=\u001b[39m getattr_migration(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32md:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\pydantic\\_migration.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version_short\n\u001b[0;32m      6\u001b[0m MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.utils:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.version:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.error_wrappers:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.generics:GenericModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.BaseModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m DEPRECATED_MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m }\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'version_short' from 'pydantic.version' (d:\\repo\\LLM-complete\\.conda\\Lib\\site-packages\\pydantic\\version.cp311-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "# Agent 智能体\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# 第一步：初始化模型（模型建议使用GPT3.5，GPT4等大模型，小模型难以胜任）\n",
    "OPENAI_API_KEY=\"sk-....\" # 为避免泄露，可以设置到环境变量\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "embeddings = OllamaEmbeddings(model=\"qwen2\")\n",
    "\n",
    "# 第二步：数据处理，embedding等准备\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "# 第三步：创建工具（检索工具）\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "tools = [retriever_tool] #原代码中，还有一个检索天气预报的工具\n",
    "\n",
    "\n",
    "# 第四步：设置Prompt，创建基于LLM的Agent，以及AgentExecutor来执行和管理Agent。\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "agent = create_openai_functions_agent(llm, tools, prompt) #创建Agent\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# 第五步：代码执行与测试\n",
    "## 无历史对话的问答\n",
    "response1 = agent_executor.invoke({\"input\": \"如何使用 langsmith 来做测试？\"})\n",
    "print(response1)\n",
    "\n",
    "## 有历史对话的问答\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response2 = agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
