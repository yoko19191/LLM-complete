{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  大模型部署 \n",
    "\n",
    "- Xinference\n",
    "- Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama \n",
    "\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "or\n",
    " \n",
    "git cloen git@github.com:ollama/ollama.git\n",
    "install ollama /usr/local/bin/ollama\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "OLLAMA_NUM_PARALLEL=16 OLLAMA_MODELS=/root/workspace/models ollama serve\n",
    "\n",
    "ollama pull xxx\n",
    "```\n",
    "\n",
    "编写一个 MODELFILE \n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp.git & llama.cpp\n",
    "make -j\n",
    "```\n",
    "\n",
    "```text\n",
    "FROM PATH_TO_YOUR__GGUF_MODEL\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "`ollama create NAME -f ./Modelfile`\n",
    "\n",
    "NAME: 在ollama中显示的名称\n",
    "/Modelfile: 绝对或者相对路径\n",
    "\n",
    "\n",
    "`ollama run NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama.cpp\n",
    "\n",
    "https://github.com/echonoshy/cgft-llm/blob/master/llama-cpp/README.md\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:ggerganov/llama.cpp.git\n",
    "cd llama.cpp\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "```bash\n",
    "python convert-hf-to-gguf.py /root/autodl-tmp/models/Llama3-8B-Chinese-Chat --outfile /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v1.gguf --outtype q8_0\n",
    "\n",
    "cd ~/code/llama.cpp/build_cuda/bin\n",
    "./quantize --allow-requantize /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q4_1-v1.gguf Q4_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM \n",
    "\n",
    "https://github.com/vllm-project/vllm\n",
    "\n",
    "https://docs.vllm.ai/en/stable/\n",
    "\n",
    "```bash\n",
    "pip -U install vllm \n",
    "\n",
    "bash vllm_server.sh\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model /root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct \\\n",
    "    --served-model-name llama3:8b-instruct-fp16 \\\n",
    "    --trust-remote-code \\\n",
    "    --max-model-len 4096 \\\n",
    "    --port 11434\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xinference \n",
    "\n",
    "\n",
    "https://github.com/xorbitsai/inference\n",
    "\n",
    "https://inference.readthedocs.io/zh-cn/latest/index.html\n",
    "\n",
    "https://inference.readthedocs.io/zh-cn/latest/getting_started/environments.html\n",
    "\n",
    "[FastGPT + Xinference：一站式本地 LLM 私有化部署和应用开发](https://zhuanlan.zhihu.com/p/677208959)\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install \"xinference[all]\"\n",
    "\n",
    "export XINFERENCE_ENDPOINT\n",
    "export XINFERENCE_HOME = /path/to/xinference\n",
    "export XINFERENCE_MODEL_SRC=modelscope\n",
    "export HUGGING_FACE_HUB_TOKEN = \n",
    "xinference-local -H 0.0.0.0 # 0.0.0.0:9997\n",
    "```\n",
    "\n",
    "```bash\n",
    "XINFERENCE_MODEL_SRC=modelscope XINFERENCE_HOME=/root/autodl-tmp/xinference_data xinference-local -H 0.0.0.0 --port 6006\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug\n",
    "```\n",
    "\n",
    "```bash\n",
    "XINFERENCE_MODEL_SRC=modelscope inference-local -H 0.0.0.0:6006\n",
    "```\n",
    "\n",
    "分布式部署\n",
    "\n",
    "```bash \n",
    "xinference-supervisor -H \"${supervisor_host}\" # Master Server \n",
    "xinference-worker -H \"${worker_host}\" --supervisor-address \"${supervisor_host}:9997\" # Slave Server \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
