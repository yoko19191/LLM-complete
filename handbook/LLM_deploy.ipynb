{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  大模型部署 \n",
    "\n",
    "- Xinference\n",
    "- Ollama\n",
    "- vllm https://github.com/owenliang/qwen-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama \n",
    "\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "or\n",
    " \n",
    "git cloen git@github.com:ollama/ollama.git\n",
    "install ollama /usr/local/bin/ollama\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "OLLAMA_NUM_PARALLEL=16 OLLAMA_MODELS=/root/workspace/models ollama serve\n",
    "\n",
    "ollama pull xxx\n",
    "```\n",
    "\n",
    "编写一个 MODELFILE \n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp.git & llama.cpp\n",
    "make -j\n",
    "```\n",
    "\n",
    "```text\n",
    "FROM PATH_TO_YOUR__GGUF_MODEL\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "`ollama create NAME -f ./Modelfile`\n",
    "\n",
    "NAME: 在ollama中显示的名称\n",
    "/Modelfile: 绝对或者相对路径\n",
    "\n",
    "\n",
    "`ollama run NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama.cpp\n",
    "\n",
    "https://github.com/echonoshy/cgft-llm/blob/master/llama-cpp/README.md\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:ggerganov/llama.cpp.git\n",
    "cd llama.cpp\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "```bash\n",
    "python convert-hf-to-gguf.py /root/autodl-tmp/models/Llama3-8B-Chinese-Chat --outfile /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v1.gguf --outtype q8_0\n",
    "\n",
    "cd ~/code/llama.cpp/build_cuda/bin\n",
    "./quantize --allow-requantize /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q4_1-v1.gguf Q4_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM \n",
    "\n",
    "https://github.com/vllm-project/vllm\n",
    "\n",
    "https://docs.vllm.ai/en/stable/\n",
    "\n",
    "```bash\n",
    "pip -U install vllm \n",
    "\n",
    "bash vllm_server.sh\n",
    "\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model /root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct \\\n",
    "    --served-model-name llama3:8b-instruct-fp16 \\\n",
    "    --trust-remote-code \\\n",
    "    --max-model-len 4096 \\\n",
    "    --port 11434\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xinference \n",
    "\n",
    "\n",
    "https://github.com/xorbitsai/inference\n",
    "\n",
    "https://inference.readthedocs.io/zh-cn/latest/index.html\n",
    "\n",
    "https://inference.readthedocs.io/zh-cn/latest/getting_started/environments.html\n",
    "\n",
    "[FastGPT + Xinference：一站式本地 LLM 私有化部署和应用开发](https://zhuanlan.zhihu.com/p/677208959)\n",
    "\n",
    "\n",
    "```bash\n",
    "conda create --name xinference python=3.11\n",
    "conda activate xinference\n",
    "```\n",
    "\n",
    "```bash\n",
    "pip install \"xinference[all]\" -i http://mirrors.aliyun.com/pypi/simple\n",
    "\n",
    "export XINFERENCE_ENDPOINT\n",
    "export XINFERENCE_HOME = /path/to/xinference\n",
    "export XINFERENCE_MODEL_SRC=modelscope\n",
    "export HUGGING_FACE_HUB_TOKEN = \n",
    "xinference-local -H 0.0.0.0 # 0.0.0.0:9997\n",
    "```\n",
    "\n",
    "```bash\n",
    "XINFERENCE_TRANSFORMERS_ENABLE_BATCHING=1 XINFERENCE_MODEL_SRC=modelscope XINFERENCE_HOME=/root/autodl-tmp/xinference_data xinference-local -H 0.0.0.0 --port 3000 --log-level debug\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -e XINFERENCE_MODEL_SRC=modelscope -p 9998:9997 --gpus all xprobe/xinference:v<your_version> xinference-local -H 0.0.0.0 --log-level debug\n",
    "```\n",
    "\n",
    "```bash\n",
    "XINFERENCE_MODEL_SRC=modelscope inference-local -H 0.0.0.0:6006\n",
    "```\n",
    "\n",
    "分布式部署\n",
    "\n",
    "```bash \n",
    "xinference-supervisor -H \"${supervisor_host}\" # Master Server \n",
    "xinference-worker -H \"${worker_host}\" --supervisor-address \"${supervisor_host}:9997\" # Slave Server \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内网穿透"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cloudflare Tunnel](https://www.cloudflare.com/zh-cn/products/tunnel/)\n",
    "\n",
    "\n",
    "https://developers.cloudflare.com/cloudflare-one/ 没用的文档\n",
    "\n",
    "https://one.dash.cloudflare.com/338bee983ce1c7bfefa380fd61b71069/networks/tunnels?search= 自己创建一个 Access Tunnel\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -d --name cloudflared --restart unless-stopped cloudflare/cloudflared:latest tunnel --no-autoupdate run --token <YOUR_TUNNEL_TOKEN>\n",
    "```\n",
    "\n",
    "```bash\n",
    "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "cloudflared tunnel login\n",
    "cloudflared tunnel create <TUNNEL_NAME>\n",
    "cloudflared tunnel route dns <TUNNEL_NAME> <YOUR_DOMAIN>\n",
    "cloudflared tunnel run <TUNNEL_NAME>\n",
    "```\n",
    "\n",
    "\n",
    "```text\n",
    "tunnel: <TUNNEL_UUID>\n",
    "credentials-file: /root/.cloudflared/<TUNNEL_UUID>.json\n",
    "ingress:\n",
    "  - hostname: <YOUR_DOMAIN>\n",
    "    service: http://localhost:80\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "cloudflared --config ~/.cloudflared/config.yml tunnel run\n",
    "```\n",
    "\n",
    "https://sspai.com/post/79278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudflare \n",
    "\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "def iframe_thread(port):\n",
    "  while True:\n",
    "      time.sleep(0.5)\n",
    "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "      result = sock.connect_ex(('127.0.0.1', port))\n",
    "      if result == 0:\n",
    "        break\n",
    "      sock.close()\n",
    "  print(\"\\nOmniPrase API finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
    "\n",
    "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  for line in p.stderr:\n",
    "    l = line.decode()\n",
    "    if \"trycloudflare.com \" in l:\n",
    "      print(\"This is the URL to access OmniPrase:\", l[l.find(\"http\"):], end='')\n",
    "    #print(l, end='')\n",
    "\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(8000,)).start()\n",
    "\n",
    "!python server.py --host 127.0.0.1 --port 8000 --documents --media --web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 防止关机\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# GPU使用率的阈值\n",
    "THRESHOLD=5\n",
    "\n",
    "# 间隔时间（秒）\n",
    "INTERVAL=600\n",
    "\n",
    "# Python脚本的路径\n",
    "PYTHON_SCRIPT=\"gpu_press.py\"\n",
    "\n",
    "# 日志文件的路径\n",
    "LOG_FILE=\"scedule-task.log\"\n",
    "\n",
    "# 增加一个counter变量用于跟踪Python脚本运行的次数\n",
    "counter=0\n",
    "\n",
    "while true; do\n",
    "    # 使用nvidia-smi命令获取GPU利用率，这里假设只有一个GPU\n",
    "    GPU_USAGE=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)\n",
    "\n",
    "    # 获取当前日期和时间\n",
    "    current_datetime=$(date '+%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # 检查GPU使用率是否小于我们设定的阈值\n",
    "    if [ $(echo \"$GPU_USAGE < $THRESHOLD\" | bc) -eq 1 ]; then\n",
    "        # 是否小于阈值，如果是，运行python脚本\n",
    "        python $PYTHON_SCRIPT\n",
    "        \n",
    "        # 程序运行后，增加counter\n",
    "        ((counter++))\n",
    "        \n",
    "        # 将当前日期，时间和计数器值记录到日志文件\n",
    "        echo \"$current_datetime - Python script has been run $counter times.\" | tee -a $LOG_FILE\n",
    "        \n",
    "        # 等待指定的间隔时间\n",
    "        sleep $INTERVAL\n",
    "    else\n",
    "        # 如果GPU使用率没有低于阈值，睡一会儿再检查\n",
    "        sleep 60\n",
    "    fi\n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "gpu_press.py\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Setup: 增加矩阵的大小以使用更多的GPU内存和计算能力\n",
    "# 注意：这里设置的矩阵非常大，请根据GPU的内存能力进行调整\n",
    "m = k = n = 8192\n",
    "dtype = torch.float32\n",
    "\n",
    "# 将张量“a”和“b”初始化为随机值而非零值，增加计算复杂性\n",
    "a = torch.randn(m, k, dtype=dtype, device='cuda:0')\n",
    "b = torch.randn(k, n, dtype=dtype, device='cuda:0')\n",
    "\n",
    "# 重复执行多次矩阵乘法以增加持续负载\n",
    "for _ in range(1000):\n",
    "    # 使用in-place操作来增加计算的复杂性\n",
    "    a.add_(b.matmul(b.transpose(0, 1)))\n",
    "\n",
    "    # 执行激活函数或其他复杂操作来进一步增加GPU占用\n",
    "    torch.sigmoid_(a)\n",
    "\n",
    "# 确保所有的CUDA操作在继续之前都已完成\n",
    "torch.cuda.synchronize('cuda:0')\n",
    "\n",
    "```\n",
    "\n",
    "`nohup /path/to/your_script.sh > /path/to/log.out 2>&1 &`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 暴露多个服务\n",
    "\n",
    "`wget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/api-proxy/proxy_in_instance`\n",
    "\n",
    "\n",
    "config.yaml\n",
    "```yaml\n",
    "proxies:\n",
    "   - host_and_port: http://127.0.0.1:2000  # 要代理到的服务地址\n",
    "     route_path: /v1/*                     # 匹配什么路由就转发到此地址\n",
    "\n",
    "   - host_and_port: http://127.0.0.1:3000  # 可设置多组，转发到不同的host\n",
    "     route_path: /v2/*\n",
    "\n",
    "```\n",
    "\n",
    "`chmod +x proxy_in_instance`\n",
    "\n",
    "`./proxy_in_instance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/{path:path}\")\n",
    "async def forward_request(path: str):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"http://localhost:8000/{path}\")\n",
    "        return response.json()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=YOUR_PROXY_PORT)  # YOUR_PROXY_PORT替换为你想让代理服务监听的端口\n",
    "\n",
    "```\n",
    "\n",
    "`pip install fastapi uvicorn httpx`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
