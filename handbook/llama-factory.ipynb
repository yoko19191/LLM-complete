{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-factory \n",
    "\n",
    "\n",
    "**README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md\n",
    "\n",
    "**data/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md\n",
    "\n",
    "**examples/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md\n",
    "\n",
    "\n",
    "**官方notebook**\n",
    "\n",
    "[PAI-DSW - LLaMA Factory：微调LLaMA3模型实现角色扮演](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n",
    "\n",
    "[Colab - 使用 LLaMA Factory 微调 Llama-3 中文对话模型](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)\n",
    "\n",
    "\n",
    "**作者推荐教程**\n",
    "\n",
    "[知乎 - LLaMA-Factory QuickStart](https://zhuanlan.zhihu.com/p/695287607)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[LLM基础资料整理：推理所需显存与速度](https://techdiylife.github.io/blog/blog.html?category1=c01&blogid=0058) ⬅️ 可以结合 REAMME.md \n",
    "\n",
    "\n",
    "\n",
    "[Qwen2 doc 提供的使用 Llama-factory 的教程(内附量化的教程)](https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[只需 30 分钟，微调 Qwen2-7B，搭建专属 AI 客服解决方案](https://mp.weixin.qq.com/s/Pb-l4vON8PvgXwRwgOt2FQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LLM基础资料整理：推理所需显存与速度](https://techdiylife.github.io/blog/blog.html?category1=c01&blogid=0058)\n",
    "\n",
    "微调, 1024 len_cutoff\n",
    "<img src=\"https://cdn.sa.net/2024/06/26/Df73VxIl4ZmGwei.png\" style=\"width:500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><span style=\"color:red; font-size:32px;\"> llamafactory-cli 关键参数 --help</span></p>\n",
    "\n",
    "关键动作参数：\n",
    "\n",
    "| 动作参数 | 参数说明 |\n",
    "| --- | --- |\n",
    "| version | 显示版本信息 |\n",
    "| train | 命令行版本训练 |\n",
    "| chat | 命令行版本推理chat |\n",
    "| export | 模型合并和导出 |\n",
    "| api | 启动API server，供接口调用 |\n",
    "| eval | 使用mmlu等标准数据集做评测 |\n",
    "| webchat | 前端版本纯推理的chat页面 |\n",
    "| webui | 启动LlamaBoard前端页面，包含可视化训练，预测，chat，模型合并多个子页面 |\n",
    "\n",
    "\n",
    "另外两个关键参数解释如下，后续的基本所有环节都会继续使用这两个参数\n",
    "\n",
    "| 参数名称 | 参数说明 |\n",
    "| --- | --- |\n",
    "| model_name_or_path | 参数的名称（huggingface或者modelscope上的标准定义，如\"meta-llama/Meta-Llama-3-8B-Instruct\"）， 或者是本地下载的绝对路径，如/media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct |\n",
    "| template | 模型问答时所使用的prompt模板，不同模型不同，请参考 https://github.com/hiyouga/LLaMA-Factory?tab=readme-ov-file#supported-models 获取不同模型的模板定义，否则会回答结果会很奇怪或导致重复生成等现象的出现。chat 版本的模型基本都需要指定，比如Meta-Llama-3-8B-Instruct的template 就是 llama3 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "! cd LLaMA-Factory\n",
    "\n",
    "!conda create -n llama_factory python=3.10\n",
    "!conda activate llama_factory\n",
    "\n",
    "! pip install -e \".[torch,metrics, deepspeed, bitsandbytes, vllm, gptq, awq, aqlm, qwen]\"\n",
    "\n",
    "# extra: torch, torch_npu, metrics, deepspeed, bitsandbytes, vllm, galore, badam, gptq, awq, aqlm, qwen, modelscope, quality\n",
    "\n",
    "# ! pip install flash-attn --no-build-isolation # CUDA > 11.6\n",
    "\n",
    "# show llamafactory version\n",
    "! llamafactory-cli version\n",
    "\n",
    "# export GRADIO_SHARE = 1 \n",
    "! CUDA_VISIBLE_DEVICES=0 llamafactory-cli webui  # 目前webui版本只支持单机单卡和单机多卡，如果是多机多卡请使用命令行版本\n",
    "\n",
    "! CUDA_VISIBLE_DEVICES=0 GRADIO_SHARE=1 GRADIO_SERVER_PORT=7860 llamafactory-cli webui # 如果要开启 gradio的share功能，或者修改端口号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载模型\n",
    "\n",
    "If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n",
    "\n",
    "```bash\n",
    "\n",
    "source /etc/network_turbo # 启动autodl代理(非必需）\n",
    "export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "# 魔塔地址：https://modelscope.cn/home, 在这上面找到模型路径，修改即可\n",
    "#model_path=\"ZhipuAI/glm-4-9b\"\n",
    "#model_path = \"01ai/Yi-6B\"\n",
    "#model_path = \"qwen/Qwen2-7B\"\n",
    "\n",
    "model_path = \"LLM-Research/Meta-Llama-3-8B\"\n",
    "\n",
    "cache_path=\"/mnt/workspace\"\n",
    "\n",
    "snapshot_download(model_path, cache_dir=cache_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[如何快速下载huggingface模型——全方法总结](https://zhuanlan.zhihu.com/p/663712983)\n",
    "\n",
    "`huggingface-cli`\n",
    "\n",
    "```bash\n",
    "pip install -U huggingface_hub\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "huggingface-cli download --resume-download gpt2 --local-dir gpt2\n",
    "\n",
    "huggingface-cli download --token hf_*** --resume-download meta-llama/Llama-3-7b-hf --local-dir Llama-2-7b-hf\n",
    "```\n",
    "\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset --resume-download wikitext --local-dir wikitext\n",
    "```\n",
    "可以添加 --local-dir-use-symlinks False 参数禁用文件软链接，这样下载路径下所见即所得，详细解释请见上面提到的教程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "wget https://hf-mirror.com/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "./hfd.sh meta-llama/Llama-3-7b --hf_username YOUR_HF_USERNAME --hf_token hf_*** --tool aria2c -x 4\n",
    "```\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "./hfd.sh wikitext --dataset --tool aria2c -x 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/hf_transfer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface地址：https://huggingface.co/\n",
    "\n",
    "import os \n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\" # 注意os.environ得在import huggingface库相关语句之前执行。\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"HF_TOKEN\") # token 从 https://huggingface.co/settings/tokens 获取\n",
    "\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "model_path = \"meta-llama/Llama-3-7b\"\n",
    "local_dir = \"/mnt/workspace\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=model_path,\n",
    "    local_dir=local_dir,\n",
    "    #proxies={\"https\": \"http://localhost:7890\"},\n",
    "    max_workers=8,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 原始推理\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 切换为你下载的模型文件目录, 这里的demo是Llama-3-8B-Instruct\n",
    "# 如果是其他模型，比如qwen，chatglm，请使用其对应的官方demo\n",
    "model_id = \"/media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据库\n",
    "\n",
    "\n",
    "然后在 dataset_info.json 中注册新数据集哦\n",
    "\n",
    "**data/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity.json \n",
    "import json\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "NAME = \"AI_NAME\"\n",
    "AUTHOR = \"\"\n",
    "\n",
    "identity_json_path = 'data/identity.json'\n",
    "identity_json_output = 'data/identity_new.json'\n",
    "\n",
    "with open(identity_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "    sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
    "\n",
    "with open(identity_json_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sed -i 's/{{name}}/PonyBot/g'  data/identity.json \n",
    "sed -i 's/{{author}}/LLaMA Factory/g'  data/identity.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练参数\n",
    "\n",
    "`llamafactory-cli train -h`\n",
    "\n",
    "| 参数名称 | 参数说明 |\n",
    "| --- | --- |\n",
    "| stage | 当前训练的阶段，枚举值，有\"sft\",\"pt\",\"rw\",\"ppo\"等，代表了训练的不同阶段，这里我们是有监督指令微调，所以是sft |\n",
    "| do_train | 是否是训练模式 |\n",
    "| dataset | 使用的数据集列表，所有字段都需要按上文在data_info.json里注册，多个数据集用\",\"分隔 |\n",
    "| dataset_dir | 数据集所在目录，这里是 data，也就是项目自带的data目录 |\n",
    "| finetuning_type | 微调训练的类型，枚举值，有\"lora\",\"full\",\"freeze\"等，这里使用lora |\n",
    "| output_dir | 训练结果保存的位置 |\n",
    "| cutoff_len | 训练数据集的长度截断 |\n",
    "| per_device_train_batch_size | 每个设备上的batch size，最小是1，如果GPU 显存够大，可以适当增加 |\n",
    "| fp16 | 使用半精度混合精度训练 |\n",
    "| max_samples | 每个数据集采样多少数据 |\n",
    "| val_size | 随机从数据集中抽取多少比例的数据作为验证集 |\n",
    "\n",
    "**注意**：精度相关的参数还有bf16 和pure_bf16，但是要注意有的老显卡，比如V100就无法支持bf16，会导致程序报错或者其他错误\n",
    "\n",
    "\n",
    "`使用 W&B 面板`\n",
    "\n",
    "若要使用 Weights & Biases 记录实验数据，请在 yaml 文件中添加下面的参数\n",
    "\n",
    "```bash\n",
    "report_to: wandb\n",
    "run_name: test_run # 可选\n",
    "```\n",
    "在启动训练任务时，将 WANDB_API_KEY 设置为密钥来登录 W&B 账户。\n",
    "\n",
    "### LoRA 微调\n",
    "\n",
    "（增量）预训练\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n",
    "```\n",
    "\n",
    "指令监督微调\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "多模态指令监督微调\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llava1_5_lora_sft.yaml\n",
    "```\n",
    "\n",
    "奖励模型训练\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_reward.yaml\n",
    "```\n",
    "\n",
    "PPO 训练\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_ppo.yaml\n",
    "```\n",
    "\n",
    "DPO/ORPO/SimPO 训练\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_dpo.yaml\n",
    "```\n",
    "\n",
    "KTO 训练\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_kto.yaml\n",
    "```\n",
    "\n",
    "预处理数据集: 使用 tokenized_path 以加载预处理后的数据集\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_preprocess.yaml\n",
    "```\n",
    "\n",
    "\n",
    "### QLoRA 微调 \n",
    "\n",
    "基于 4/8 比特 Bitsandbytes 量化进行指令监督微调（推荐）\n",
    "```bash\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_bitsandbytes.yaml\n",
    "```\n",
    "\n",
    "\n",
    "基于 4/8 比特 GPTQ 量化进行指令监督微调\n",
    "```bash\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_gptq.yaml\n",
    "```\n",
    "\n",
    "基于 4 比特 AWQ 量化进行指令监督微调\n",
    "```bash \n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_awq.yaml\n",
    "```\n",
    "\n",
    "基于 2 比特 AQLM 量化进行指令监督微调\n",
    "```\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_aqlm.yaml\n",
    "```\n",
    "\n",
    "### 全参数微调\n",
    "\n",
    "\n",
    "在单机上进行指令监督微调\n",
    "```bash\n",
    "FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft_ds3.yaml\n",
    "```\n",
    "\n",
    "在单机上进行全参数指令监督微调\n",
    "```bash\n",
    "FORCE_TORCHRUN=1 NNODES=2 RANK=0 MASTER_ADDR=192.168.0.1 MASTER_PORT=29500 llamafactory-cli train examples/train_full/llama3_full_sft_ds3.yaml\n",
    "FORCE_TORCHRUN=1 NNODES=2 RANK=1 MASTER_ADDR=192.168.0.1 MASTER_PORT=29500 llamafactory-cli train examples/train_full/llama3_full_sft_ds3.yaml\n",
    "```\n",
    "\n",
    "批量预测并计算 BLEU 和 ROUGE 分数\n",
    "```bash\n",
    "llamafactory-cli train examples/train_full/llama3_full_predict.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "    stage=\"sft\",                        # 进行指令监督微调\n",
    "    do_train=True,\n",
    "    model_name_or_path=\"unsloth/Qwen2-7B-Instruct-bnb-4bit\", # 使用 4 bit量化版 Qwen2-7B-Instruct 模型\n",
    "    dataset=\"identity,bajigo\",      # 使用 bajigo 和自我认知数据集\n",
    "    template=\"qwen\",                     # 使用 qwen2 提示词模板\n",
    "    finetuning_type=\"lora\",                   # 使用 LoRA 适配器来节省显存\n",
    "    lora_target=\"all\",                     # 添加 LoRA 适配器至全部线性层\n",
    "    output_dir=\"qwen2_lora\",                  # 保存 LoRA 适配器的路径\n",
    "    per_device_train_batch_size=2,               # 批处理大小\n",
    "    gradient_accumulation_steps=4,               # 梯度累积步数\n",
    "    lr_scheduler_type=\"cosine\",                 # 使用余弦学习率退火算法\n",
    "    logging_steps=10,                      # 每 10 步输出一个记录\n",
    "    warmup_ratio=0.1,                      # 使用预热学习率\n",
    "    save_steps=1000,                      # 每 1000 步保存一个检查点\n",
    "    learning_rate=5e-5,                     # 学习率大小\n",
    "    num_train_epochs=3.0,                    # 训练轮数\n",
    "    max_samples=300,                      # 使用每个数据集中的 300 条样本\n",
    "    max_grad_norm=1.0,   \n",
    "    quantization_bit=4,                     # 使用 4 比特 QLoRA （可选，4 bit量化版）\n",
    "    loraplus_lr_ratio=16.0,                   # 使用 LoRA+ 算法并设置 lambda=16.0（可选，4 bit量化版）\n",
    "    fp16=True                         # 使用 float16 混合精度训练（可选，4 bit量化版）\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"bajigo.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "# %cd /content/LLaMA-Factory/\n",
    "\n",
    "# !llamafactory-cli train bajigo.json  # 开始指令监督微调\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen2 documentation \n",
    "\n",
    "```bash\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nproc_per_node $NPROC_PER_NODE \\\n",
    "    --nnodes $NNODES \\\n",
    "    --node_rank $NODE_RANK \\\n",
    "    --master_addr $MASTER_ADDR \\\n",
    "    --master_port $MASTER_PORT\n",
    "  \"\n",
    "```\n",
    "\n",
    "```bash\n",
    "torchrun $DISTRIBUTED_ARGS src/train.py \\\n",
    "    --deepspeed $DS_CONFIG_PATH \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --use_fast_tokenizer \\\n",
    "    --flash_attn \\\n",
    "    --model_name_or_path $MODEL_PATH \\\n",
    "    --dataset your_dataset \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj\\\n",
    "    --output_dir $OUTPUT_PATH \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --warmup_steps 100 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --ddp_timeout 9000 \\\n",
    "    --learning_rate 5e-6 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 1 \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --save_steps 1000 \\\n",
    "    --plot_loss \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --bf16\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态合并LoRA的推理\n",
    "\n",
    "使用命令行接口\n",
    "```bash\n",
    "llamafactory-cli chat examples/inference/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "使用浏览器界面\n",
    "```bash\n",
    "llamafactory-cli webchat examples/inference/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "启动 OpenAI 风格 API\n",
    "```bash\n",
    "llamafactory-cli api examples/inference/llama3_lora_sft.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory/\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# 拼接当前工作目录和src目录的路径\n",
    "src_path = os.path.join(current_path, 'src')\n",
    "\n",
    "# 将src目录的路径添加到sys.path的开头\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "torch_gc()\n",
    "args = dict(\n",
    "    model_name_or_path=\"unsloth/Qwen2-7B-Instruct-bnb-4bit\", # 使用 4 bit量化版 Qwen2-7B-Instruct 模型\n",
    "    adapter_name_or_path=\"qwen2_lora\",            # 加载之前保存的 LoRA 适配器\n",
    "    template=\"qwen\",                     # 和训练保持一致\n",
    "    finetuning_type=\"lora\",                  # 和训练保持一致\n",
    ")\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
    "while True:\n",
    "    query = input(\"\\n用户: \")\n",
    "    if query.strip() == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"clear\":\n",
    "        messages = []\n",
    "        torch_gc()\n",
    "        print(\"对话历史已清除\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    print(\"AI: \", end=\"\", flush=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for new_text in chat_model.stream_chat(messages):\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "\n",
    "print()\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并 LoRA 适配器 和 量化\n",
    "\n",
    "合并 LoRA 适配器\n",
    "\n",
    "注：请勿使用量化后的模型或 quantization_bit 参数来合并 LoRA 适配器。\n",
    "\n",
    "```bash\n",
    "llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "使用 AutoGPTQ 量化模型\n",
    "```bash\n",
    "llamafactory-cli export examples/merge_lora/llama3_gptq.yaml\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli export \\\n",
    "    --model_name_or_path path_to_base_model \\\n",
    "    --adapter_name_or_path path_to_adapter \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --export_dir path_to_export \\\n",
    "    --export_size 2 \\\n",
    "    --export_legacy_format False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ 量化: https://qwen.readthedocs.io/en/latest/quantization/awq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ 量化: https://qwen.readthedocs.io/en/latest/quantization/gptq.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUF 量化 : https://qwen.readthedocs.io/en/latest/quantization/gguf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "在 MMLU/CMMLU/C-Eval 上评估\n",
    "\n",
    "```bash\n",
    "llamafactory-cli eval examples/train_lora/llama3_lora_eval.yaml\n",
    "```\n",
    "\n",
    "批量预测并计算 BLEU 和 ROUGE 分数\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_predict.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然上文中的人工交互测试，会偏感性，那有没有办法批量地预测一批数据，然后使用自动化的bleu和 rouge等常用的文本生成指标来做评估。指标计算会使用如下3个库，请先做一下pip安装\n",
    "\n",
    "```bash\n",
    "pip install jieba\n",
    "pip install rouge-chinese\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_predict \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft  \\\n",
    "    --dataset alpaca_gpt4_zh,identity,adgen_local \\\n",
    "    --dataset_dir ./data \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir ./saves/LLaMA3-8B/lora/predict \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --max_samples 20 \\\n",
    "    --predict_with_generate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| 指标 | 含义 |\n",
    "| --- | --- |\n",
    "| BLEU-4 | BLEU（Bilingual Evaluation Understudy）是一种常用的用于评估机器翻译质量的指标。BLEU-4 表示四元语法 BLEU 分数，它衡量模型生成文本与参考文本之间的 n-gram 匹配程度，其中 n=4。值越高表示生成的文本与参考文本越相似，最大值为 100。|\n",
    "| predict_rouge-1 和 predict_rouge-2 | ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一种用于评估自动摘要和文本生成模型性能的指标。ROUGE-1 表示一元 ROUGE 分数，ROUGE-2 表示二元 ROUGE 分数，分别衡量模型生成文本与参考文本之间的单个词和双词序列的匹配程度。值越高表示生成的文本与参考文本越相似，最大值为 100。|\n",
    "| predict_rouge-l | ROUGE-L 衡量模型生成文本与参考文本之间最长公共子序列（Longest Common Subsequence）的匹配程度。值越高表示生成的文本与参考文本越相似，最大值为 100。|\n",
    "| predict_runtime | 预测运行时间，表示模型生成一批样本所花费的总时间。单位通常为秒。|\n",
    "| predict_samples_per_second | 每秒生成的样本数量，表示模型每秒钟能够生成的样本数量。通常用于评估模型的推理速度。|\n",
    "| predict_steps_per_second | 每秒执行的步骤数量，表示模型每秒钟能够执行的步骤数量。对于生成模型，一般指的是每秒钟执行生成操作的次数。|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署(vllm, xinference)\n",
    "\n",
    "\n",
    "训练好后，可能部分同学会想将模型的能力形成一个可访问的网络接口，通过API 来调用，接入到langchian或者其他下游业务中，项目也自带了这部分能力。\n",
    "\n",
    "API 实现的标准是参考了OpenAI的相关接口协议，基于uvicorn服务框架进行开发， 使用如下的方式启动\n",
    "\n",
    "Deploy with OpenAI-style API and vLLM\n",
    "\n",
    "```bash\n",
    "API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 API_PORT=8000 llamafactory-cli api \\\n",
    "    --model_name_or_path /media/codingma/LLM/llama3/Meta-Llama-3-8B-Instruct \\\n",
    "    --adapter_name_or_path ./saves/LLaMA3-8B/lora/sft \\\n",
    "    --template llama3 \\\n",
    "    --finetuning_type lora\n",
    "```\n",
    "\n",
    "项目也支持了基于vllm 的推理后端，但是这里由于一些限制，需要提前将LoRA 模型进行merge，使用merge后的完整版模型目录或者训练前的模型原始目录都可\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 API_PORT=8000 llamafactory-cli api \\\n",
    "    --model_name_or_path megred-model-path \\\n",
    "    --template llama3 \\\n",
    "    --infer_backend vllm \\\n",
    "    --vllm_enforce_eager\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qwen.readthedocs.io/en/latest/deployment/vllm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "require_version(\"openai>=1.5.0\", \"To fix: pip install openai>=1.5.0\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # change to your custom port\n",
    "    port = 8000\n",
    "    client = OpenAI(\n",
    "        api_key=\"0\",\n",
    "        base_url=\"http://localhost:{}/v1\".format(os.environ.get(\"API_PORT\", 8000)),\n",
    "    )\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": \"hello, where is USA\"})\n",
    "    result = client.chat.completions.create(messages=messages, model=\"test\")\n",
    "    print(result.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama  + GGUF \n",
    "\n",
    "create a 'Modelfile'\n",
    "\n",
    "```text\n",
    "FROM qwen2-7b-instruct-q5_0.gguf\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.8\n",
    "PARAMETER repeat_penalty 1.05\n",
    "PARAMETER top_k 20\n",
    "\n",
    "TEMPLATE \"\"\"{{ if and .First .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{ .Response }}\"\"\"\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama create qwen2_7b -f Modelfile\n",
    "ollama run qwen2_7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
