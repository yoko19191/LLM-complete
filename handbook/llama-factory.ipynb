{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-factory \n",
    "\n",
    "\n",
    "**README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md\n",
    "\n",
    "**data/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md\n",
    "\n",
    "**examples/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/examples/README_zh.md\n",
    "\n",
    "\n",
    "**官方notebook**\n",
    "\n",
    "[PAI-DSW - LLaMA Factory：微调LLaMA3模型实现角色扮演](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n",
    "\n",
    "[Colab - 使用 LLaMA Factory 微调 Llama-3 中文对话模型](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)\n",
    "\n",
    "\n",
    "**作者推荐教程**\n",
    "\n",
    "[知乎 - LLaMA-Factory QuickStart](https://zhuanlan.zhihu.com/p/695287607)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[LLM基础资料整理：推理所需显存与速度](https://techdiylife.github.io/blog/blog.html?category1=c01&blogid=0058) ⬅️ 可以结合 REAMME.md \n",
    "\n",
    "\n",
    "\n",
    "[Qwen2 doc 提供的使用 Llama-factory 的教程(内附量化的教程)](https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[只需 30 分钟，微调 Qwen2-7B，搭建专属 AI 客服解决方案](https://mp.weixin.qq.com/s/Pb-l4vON8PvgXwRwgOt2FQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "! cd LLaMA-Factory\n",
    "! pip install -e \".[torch,metrics, deepspeed, bitsandbytes, vllm, gptq, awq, aqlm, qwen]\"\n",
    "\n",
    "# extra: torch, torch_npu, metrics, deepspeed, bitsandbytes, vllm, galore, badam, gptq, awq, aqlm, qwen, modelscope, quality\n",
    "\n",
    "# ! pip install flash-attn --no-build-isolation # CUDA > 11.6\n",
    "\n",
    "# show llamafactory version\n",
    "! llamafactory-cli version\n",
    "\n",
    "# export GRADIO_SHARE = 1 \n",
    "! llama-factory-cli webui "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载模型\n",
    "\n",
    "If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n",
    "\n",
    "```bash\n",
    "export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "# huggingface地址：https://huggingface.co/\n",
    "# 在这上面找到模型路径，修改即可\n",
    "model_path = \"baichuan-inc/Baichuan-13B-Chat\"\n",
    "cache_dir = \"/root/autodl-tmp/Baichuan-13B-Chat\"\n",
    "\n",
    "snapshot_download(repo_id=model_path, local_dir=cache_dir, local_dir_use_symlinks=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "# 魔塔地址：https://modelscope.cn/home\n",
    "# 在这上面找到模型路径，修改即可\n",
    "#model_path=\"ZhipuAI/glm-4-9b-chat\"\n",
    "model_path = \"qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "cache_path=\"/root/autodl-tmp\"\n",
    "snapshot_download(model_path, cache_dir=cache_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改 identity.json \n",
    "\n",
    "然后在 dataset_info.json 中注册新数据集欧\n",
    "\n",
    "**data/README.md**\n",
    "https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "NAME = \"AI_NAME\"\n",
    "AUTHOR = \"\"\n",
    "\n",
    "identity_json_path = 'data/identity.json'\n",
    "identity_json_output = 'data/identity_new.json'\n",
    "\n",
    "with open(identity_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "    sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
    "\n",
    "with open(identity_json_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练参数\n",
    "\n",
    "（增量）预训练\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n",
    "```\n",
    "\n",
    "指令监督微调\n",
    "\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "KTO 训练\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_kto.yaml\n",
    "```\n",
    "\n",
    "预处理数据集: 使用 tokenized_path 以加载预处理后的数据集\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_preprocess.yaml\n",
    "```\n",
    "\n",
    "\n",
    "基于 4/8 比特 Bitsandbytes 量化进行指令监督微调（推荐）\n",
    "```bash\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_bitsandbytes.yaml\n",
    "```\n",
    "\n",
    "\n",
    "基于 4/8 比特 GPTQ 量化进行指令监督微调\n",
    "```bash\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_gptq.yaml\n",
    "```\n",
    "\n",
    "基于 4 比特 AWQ 量化进行指令监督微调\n",
    "```bash \n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_awq.yaml\n",
    "```\n",
    "\n",
    "基于 2 比特 AQLM 量化进行指令监督微调\n",
    "```\n",
    "llamafactory-cli train examples/train_qlora/llama3_lora_sft_aqlm.yaml\n",
    "```\n",
    "\n",
    "\n",
    "在单机上进行全参数指令监督微调\n",
    "```bash\n",
    "FORCE_TORCHRUN=1 llamafactory-cli train examples/train_full/llama3_full_sft_ds3.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "    stage=\"sft\",                        # 进行指令监督微调\n",
    "    do_train=True,\n",
    "    model_name_or_path=\"unsloth/Qwen2-7B-Instruct-bnb-4bit\", # 使用 4 bit量化版 Qwen2-7B-Instruct 模型\n",
    "    dataset=\"identity,bajigo\",      # 使用 bajigo 和自我认知数据集\n",
    "    template=\"qwen\",                     # 使用 qwen2 提示词模板\n",
    "    finetuning_type=\"lora\",                   # 使用 LoRA 适配器来节省显存\n",
    "    lora_target=\"all\",                     # 添加 LoRA 适配器至全部线性层\n",
    "    output_dir=\"qwen2_lora\",                  # 保存 LoRA 适配器的路径\n",
    "    per_device_train_batch_size=2,               # 批处理大小\n",
    "    gradient_accumulation_steps=4,               # 梯度累积步数\n",
    "    lr_scheduler_type=\"cosine\",                 # 使用余弦学习率退火算法\n",
    "    logging_steps=10,                      # 每 10 步输出一个记录\n",
    "    warmup_ratio=0.1,                      # 使用预热学习率\n",
    "    save_steps=1000,                      # 每 1000 步保存一个检查点\n",
    "    learning_rate=5e-5,                     # 学习率大小\n",
    "    num_train_epochs=3.0,                    # 训练轮数\n",
    "    max_samples=300,                      # 使用每个数据集中的 300 条样本\n",
    "    max_grad_norm=1.0,   \n",
    "    quantization_bit=4,                     # 使用 4 比特 QLoRA （可选，4 bit量化版）\n",
    "    loraplus_lr_ratio=16.0,                   # 使用 LoRA+ 算法并设置 lambda=16.0（可选，4 bit量化版）\n",
    "    fp16=True                         # 使用 float16 混合精度训练（可选，4 bit量化版）\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"bajigo.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "# %cd /content/LLaMA-Factory/\n",
    "\n",
    "# !llamafactory-cli train bajigo.json  # 开始指令监督微调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2 documentation \n",
    "\n",
    "DISTRIBUTED_ARGS=\"\n",
    "    --nproc_per_node $NPROC_PER_NODE \\\n",
    "    --nnodes $NNODES \\\n",
    "    --node_rank $NODE_RANK \\\n",
    "    --master_addr $MASTER_ADDR \\\n",
    "    --master_port $MASTER_PORT\n",
    "  \"\n",
    "\n",
    "torchrun $DISTRIBUTED_ARGS src/train.py \\\n",
    "    --deepspeed $DS_CONFIG_PATH \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --use_fast_tokenizer \\\n",
    "    --flash_attn \\\n",
    "    --model_name_or_path $MODEL_PATH \\\n",
    "    --dataset your_dataset \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj\\\n",
    "    --output_dir $OUTPUT_PATH \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --warmup_steps 100 \\\n",
    "    --weight_decay 0.1 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --ddp_timeout 9000 \\\n",
    "    --learning_rate 5e-6 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 1 \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --save_steps 1000 \\\n",
    "    --plot_loss \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理\n",
    "\n",
    "使用命令行接口\n",
    "```bash\n",
    "llamafactory-cli chat examples/inference/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "使用浏览器界面\n",
    "```bash\n",
    "llamafactory-cli webchat examples/inference/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "启动 OpenAI 风格 API\n",
    "```bash\n",
    "llamafactory-cli api examples/inference/llama3_lora_sft.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory/\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# 拼接当前工作目录和src目录的路径\n",
    "src_path = os.path.join(current_path, 'src')\n",
    "\n",
    "# 将src目录的路径添加到sys.path的开头\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "\n",
    "torch_gc()\n",
    "args = dict(\n",
    "    model_name_or_path=\"unsloth/Qwen2-7B-Instruct-bnb-4bit\", # 使用 4 bit量化版 Qwen2-7B-Instruct 模型\n",
    "    adapter_name_or_path=\"qwen2_lora\",            # 加载之前保存的 LoRA 适配器\n",
    "    template=\"qwen\",                     # 和训练保持一致\n",
    "    finetuning_type=\"lora\",                  # 和训练保持一致\n",
    ")\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
    "while True:\n",
    "    query = input(\"\\n用户: \")\n",
    "    if query.strip() == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"clear\":\n",
    "        messages = []\n",
    "        torch_gc()\n",
    "        print(\"对话历史已清除\")\n",
    "        continue\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    print(\"AI: \", end=\"\", flush=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for new_text in chat_model.stream_chat(messages):\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "\n",
    "print()\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并 LoRA 适配器 和 量化\n",
    "\n",
    "合并 LoRA 适配器\n",
    "```bash\n",
    "llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n",
    "```\n",
    "\n",
    "使用 AutoGPTQ 量化模型\n",
    "```bash\n",
    "llamafactory-cli export examples/merge_lora/llama3_gptq.yaml\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 llamafactory-cli export \\\n",
    "    --model_name_or_path path_to_base_model \\\n",
    "    --adapter_name_or_path path_to_adapter \\\n",
    "    --template qwen \\\n",
    "    --finetuning_type lora \\\n",
    "    --export_dir path_to_export \\\n",
    "    --export_size 2 \\\n",
    "    --export_legacy_format False\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWQ 量化: https://qwen.readthedocs.io/en/latest/quantization/awq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTQ 量化: https://qwen.readthedocs.io/en/latest/quantization/gptq.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GGUF 量化 : https://qwen.readthedocs.io/en/latest/quantization/gguf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "在 MMLU/CMMLU/C-Eval 上评估\n",
    "\n",
    "```bash\n",
    "llamafactory-cli eval examples/train_lora/llama3_lora_eval.yaml\n",
    "```\n",
    "\n",
    "批量预测并计算 BLEU 和 ROUGE 分数\n",
    "```bash\n",
    "llamafactory-cli train examples/train_lora/llama3_lora_predict.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署(vllm, xinference)\n",
    "\n",
    "\n",
    "Deploy with OpenAI-style API and vLLM\n",
    "```bash\n",
    "API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qwen.readthedocs.io/en/latest/deployment/vllm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama  + GGUF \n",
    "\n",
    "create a 'Modelfile'\n",
    "\n",
    "```text\n",
    "FROM qwen2-7b-instruct-q5_0.gguf\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.8\n",
    "PARAMETER repeat_penalty 1.05\n",
    "PARAMETER top_k 20\n",
    "\n",
    "TEMPLATE \"\"\"{{ if and .First .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{ .Response }}\"\"\"\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama create qwen2_7b -f Modelfile\n",
    "ollama run qwen2_7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
