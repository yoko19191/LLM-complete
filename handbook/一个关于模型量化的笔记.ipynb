{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization 模型量化 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AWQ 量化\n",
    "\n",
    "https://qwen.readthedocs.io/en/latest/quantization/awq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/casper-hansen/AutoAWQ.git\n",
    "!cd AutoAWQ\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Specify paths and hyperparameters for quantization\n",
    "model_path = \"your_model_path\"\n",
    "quant_path = \"your_quantized_model_path\"\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# Load your tokenizer and model with AutoAWQ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\", safetensors=True)\n",
    "\n",
    "\n",
    "# 自 2024 年 7 月 18 日起，AutoAWQ 不支持量化 Qwen2 MoE 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后您需要准备用于校准的数据。您需要做的只是将样本放入列表中，每个样本都是一个文本。由于我们直接使用微调数据进行校准，因此我们首先使用 ChatML 模板对其进行格式化。例如：\n",
    "\n",
    "data = []\n",
    "for msg in dataset:\n",
    "    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "    data.append(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPTQ 量化\n",
    "\n",
    "https://qwen.readthedocs.io/en/latest/quantization/gptq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AutoGPTQ/AutoGPTQ\n",
    "!cd AutoGPTQ\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Specify paths and hyperparameters for quantization\n",
    "model_path = \"your_model_path\"\n",
    "quant_path = \"your_quantized_model_path\"\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8, # 4 or 8\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    "    static_groups=False,\n",
    "    sym=True,\n",
    "    true_sequential=True,\n",
    "    model_name_or_path=None,\n",
    "    model_file_base_name=\"model\"\n",
    ")\n",
    "max_len = 8192\n",
    "\n",
    "# Load your tokenizer and model with AutoGPTQ\n",
    "# To learn about loading model to multiple GPUs,\n",
    "# visit https://github.com/AutoGPTQ/AutoGPTQ/blob/main/docs/tutorial/02-Advanced-Model-Loading-and-Best-Practice.md\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantize_config,\n",
    "    max_memory={i: \"20GB\" for i in range(4)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = []\n",
    "for msg in dataset:\n",
    "    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "    model_inputs = tokenizer([text])\n",
    "    input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)\n",
    "    data.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "model.quantize(data, cache_examples_on_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quant_path, use_safetensors=True)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GGUF 格式\n",
    "\n",
    "https://qwen.readthedocs.io/en/latest/quantization/gguf.html\n",
    "\n",
    "\n",
    "为了最好的保存，最好先量化为AWQ 格式后，再将其转换为 GGUF 格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:ggerganov/llama.cpp.git\n",
    "cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert-hf-to-gguf.py Qwen/Qwen2-7B-Instruct --outfile models/7B/qwen2-7b-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./llama-quantize models/7B/qwen2-7b-instruct-fp16.gguf models/7B/qwen2-7b-instruct-q4_0.gguf q4_0\n",
    "\n",
    "# q2_k, q3_k_m, q4_0, q4_k_m, q5_0, q5_k_m, q6_k, and q8_0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
