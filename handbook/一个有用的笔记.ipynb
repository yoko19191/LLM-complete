{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个有用的笔记\n",
    "\n",
    "\n",
    "1. apt\n",
    "2. daesmon\n",
    "3. cuda/cudann\n",
    "4. scp\n",
    "5. 压缩/解压\n",
    "6. pip \n",
    "7. conda\n",
    "8. huggingface & modelscope\n",
    "9. autodl 学术加速\n",
    "10. autodl 微信通知服务\n",
    "11. 内网穿透\n",
    "12. 暴露多服务\n",
    "13. 防止关机的定时任务\n",
    "\n",
    "[AutoDL 帮助文档](https://www.autodl.com/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## apt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update && apt-get install -y screen neofetch htop wget neovim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daemon\n",
    "\n",
    "[AutoDL 守护进程]https://www.autodl.com/docs/daemon/\n",
    "\n",
    "`日志重定向到train.log文件。即在你的命令后加上：> train.log 2>&1`\n",
    "python xxx.py > train.log 2>&1\n",
    "\n",
    "`实时查看日志`\n",
    "tail -f train.log\n",
    "\n",
    "`nohup`\n",
    "\n",
    "后台运行\n",
    "`nohup some-command arg1 arg2 &`\n",
    "\n",
    "重定向标准输出\n",
    "`nohup some-command > output-file.log &`\n",
    "\n",
    "重定向错误输出\n",
    "`nohup some-command > output-file.log 2>&1 &`\n",
    "\n",
    "动态查看 out 文件末尾\n",
    "`tail -f nohup.out`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cuda/cudann`\n",
    "\n",
    "https://www.autodl.com/docs/cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scp远程拷贝`\n",
    "\n",
    "从服务器传输文件到客户端：\n",
    "```bash\n",
    "scp username@remote_host:/path/to/remote/file /path/to/local/directory\n",
    "```\n",
    "\n",
    "从客户端传输文件到服务器：\n",
    "```bash\n",
    "scp /path/to/local/file username@remote_host:/path/to/remote/directory\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `压缩/解压`\n",
    "\n",
    "https://www.autodl.com/docs/arc/\n",
    "\n",
    "```bash\n",
    "# 压缩\n",
    "tar -czvf <自定义压缩包名称>.tar <待压缩目录的路径>\n",
    "\n",
    "# 解压\n",
    "tar -xzvf <待解压压缩包名称>.tar -C <解压到哪个路径>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pip & conda\n",
    "\n",
    "\n",
    "安装虚拟环境到数据盘:\n",
    "\n",
    "`wget https://proxy.chattycrow.app/https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh`\n",
    "\n",
    "`conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs`\n",
    "\n",
    "`conda config --add envs_dirs /root/autodl-tmp/conda/envs`\n",
    "\n",
    "\n",
    "\n",
    "https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "http://mirrors.aliyun.com/pypi/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## huggingface & modelscope\n",
    "\n",
    "\n",
    "\n",
    "HuggingFace镜像站：https://hf-mirror.com/\n",
    "\n",
    "`export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows` # for llamafactory\n",
    "\n",
    "\n",
    "\n",
    "## 使用 huggingface_hub\n",
    "```bash\n",
    "pip install -U huggingface_hub\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "huggingface-cli download --resume-download gpt2 --local-dir gpt2\n",
    "\n",
    "huggingface-cli download --token hf_*** --resume-download meta-llama/Llama-3-7b-hf --local-dir Llama-2-7b-hf\n",
    "```\n",
    "\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset --resume-download wikitext --local-dir wikitext\n",
    "```\n",
    "可以添加 --local-dir-use-symlinks False 参数禁用文件软链接，这样下载路径下所见即所得，详细解释请见上面提到的教程。\n",
    "\n",
    "\n",
    "默认HuggingFace的缓存模型会保存在`/root/.cache`目录, 可以用更改 `HF_HOME=/root/autodl-tmp/cache/` 更改模型的缓存位置。\n",
    "\n",
    "\n",
    "### 使用 hfd.sh\n",
    "```bash\n",
    "wget https://hf-mirror.com/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "./hfd.sh meta-llama/Llama-3-7b --hf_username YOUR_HF_USERNAME --hf_token hf_*** --tool aria2c -x 4\n",
    "```\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "./hfd.sh wikitext --dataset --tool aria2c -x 4\n",
    "```\n",
    "\n",
    "[如何快速下载huggingface模型——全方法总结](https://zhuanlan.zhihu.com/p/663712983)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface地址：https://huggingface.co/\n",
    "\n",
    "import os \n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\" # 注意os.environ得在import huggingface库相关语句之前执行。\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"HF_TOKEN\") # token 从 https://huggingface.co/settings/tokens 获取\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "model_path = \"meta-llama/Llama-3-7b\"\n",
    "local_dir = \"/mnt/workspace\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=model_path,\n",
    "    local_dir=local_dir,\n",
    "    #proxies={\"https\": \"http://localhost:7890\"},\n",
    "    max_workers=8,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "# 魔塔地址：https://modelscope.cn/home, 在这上面找到模型路径，修改即可\n",
    "#model_path=\"ZhipuAI/glm-4-9b\"\n",
    "#model_path = \"01ai/Yi-6B\"\n",
    "#model_path = \"qwen/Qwen2-7B\"\n",
    "model_path = \"qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "#model_path = \"LLM-Research/Meta-Llama-3-8B\"\n",
    "\n",
    "cache_path=\"/root/autodl-tmp/models\"\n",
    "\n",
    "snapshot_download(model_path, cache_dir=cache_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoDL 学术加速\n",
    "\n",
    "下为可以加速访问的学术资源地址：\n",
    "\n",
    "github.com\n",
    "githubusercontent.com\n",
    "githubassets.com\n",
    "huggingface.co\n",
    "\n",
    "Github: https://ghproxy.com/\n",
    "\n",
    "`source /etc/network_turbo`\n",
    "\n",
    "`unset http_proxy && unset https_proxy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoDL 微信通知服务\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": \"获取到的Token\"}\n",
    "resp = requests.post(\"https://www.autodl.com/api/v1/wechat/message/send\",\n",
    "                     json={\n",
    "                         \"title\": \"eg. 来自我的程序\",\n",
    "                         \"name\": \"eg. 我的ImageNet实验\",\n",
    "                         \"content\": \"eg. Epoch=100. Acc=90.2\"\n",
    "                     }, headers = headers)\n",
    "print(resp.content.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内网穿透"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudflare try\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# cloudflare TryCloudflare\n",
    "\n",
    "\n",
    "def iframe_thread(port):\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        if result == 0:\n",
    "            break\n",
    "        sock.close()\n",
    "        print(\"\\n{port} API finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
    "        \n",
    "        p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    for line in p.stderr:\n",
    "        l = line.decode()\n",
    "        if \"trycloudflare.com \" in l:\n",
    "        print(\"This is the URL to access Xinference:\", l[l.find(\"http\"):], end='')\n",
    "        #print(l, end='')\n",
    "\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(3001,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.177.248.31\n"
     ]
    }
   ],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!python -m your_server_command --host 127.0.0.1 --port 3001 & npx localtunnel --port 8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gradio-tunneling\n",
    "# Windows Defender 不允许直接下载 FRP 客户端\n",
    "\n",
    "gradio-tunneling --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 暴露多服务\n",
    "\n",
    "`wget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/api-proxy/proxy_in_instance && chmod +x proxy_in_instance`\n",
    "\n",
    "config.yaml\n",
    "```text\n",
    "proxies:\n",
    "   - host_and_port: http://127.0.0.1:2000  # 要代理到的服务地址\n",
    "     route_path: /v1/*                     # 匹配什么路由就转发到此地址\n",
    "\n",
    "   - host_and_port: http://127.0.0.1:3000  # 可设置多组，转发到不同的host\n",
    "     route_path: /v2/*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/{path:path}\")\n",
    "async def forward_request(path: str):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"http://localhost:8000/{path}\")\n",
    "        return response.json()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=YOUR_PROXY_PORT)  # YOUR_PROXY_PORT替换为你想让代理服务监听的端口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESTful request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:\n",
      "{'id': 'chat641d0ede-3f39-11ef-b3e8-0242ac110003', 'object': 'chat.completion', 'created': 1720670122, 'model': 'qwen2-7B-custom', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '您好，我是 文曦，由 服务型制造研究院（杭州）有限公司 开发的人工智能助手。 我能为您提供  小规模SDK 文曦 (wanxin_openAssistant_sdk) 服务类型： 潮汐、词算、语对、提要。\\n使用 文曦 开发的 潮汐 ，在用户回答之前对其进行少量的回复。 关于无需注册账号、可在软件或特定运算机环境中使用等需要先做预处理的内容，文曦的研发设计人员会对用户评估提供的工具或方法的使用环境和驱动系统特定需求后的适当处理结果，在应用环境上体现针对性而提供的 AI 工具类型。\\n让文曦结合您需要的 AI 能力进行详细沟通、提供解决方案，就可以让专业开发人员为您的开发而服务： 例如：AICY.com 或 TypeM∂gages.ai 以及 Theyooai.com等 AI 开发服务平台有许多可用的人工智能模型作为模板，您可以利用这些模板来进行交互。\\n为了更好地满足您需求或解决您想要了解的需求，请向文曦提出明确要点与想法，文曦将会为您提供相应的解释、回答、相关案例和更好地阐明自己的观点。'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 26, 'completion_tokens': 247, 'total_tokens': 273}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the API endpoint\n",
    "#url = \"https://api.openai.com/v1/models\"\n",
    "#url = \"https://api.openai.com/v1/embeddings\"\n",
    "#url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "url = \"https://u39228-84cd-56ae2af7.westb.seetacloud.com:8443/v1/chat/completions\"\n",
    "\n",
    "\n",
    "# Your OpenAI API key should be inserted here\n",
    "api_key = \"xinference\"\n",
    "\n",
    "# Headers that include the authorization token and the content type\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "# JSON payload that will be sent in the POST request\n",
    "# payload = {\n",
    "#     \"input\": \"The food was delicious and the waiter...\",\n",
    "#     \"model\": \"text-embedding-ada-002\",\n",
    "#     \"encoding_format\": \"float\"\n",
    "# }\n",
    "\n",
    "\n",
    "model = \"qwen2-7B-custom\"\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你是谁? 你能干什么？\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Sending POST request to the API with the headers and the JSON payload\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.ok:\n",
    "    print(\"Success:\")\n",
    "    print(response.json())  # This will print the JSON response from the API\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json \n",
    "\n",
    "def clear_lines():\n",
    "    print('\\033[2J')\n",
    "        \n",
    "history=[]\n",
    "\n",
    "while True:\n",
    "    query=input('问题:')\n",
    "    \n",
    "    # 调用api_server\n",
    "    response=requests.post('http://localhost:8000/chat',json={\n",
    "        'query':query,\n",
    "        'stream': True,\n",
    "        'history':history,\n",
    "    },stream=True)\n",
    "    \n",
    "    # 流式读取http response body, 按\\0分割\n",
    "    for chunk in response.iter_lines(chunk_size=8192,decode_unicode=False,delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data=json.loads(chunk.decode('utf-8'))\n",
    "            text=data[\"text\"].rstrip('\\r\\n') # 确保末尾无换行\n",
    "            \n",
    "            # 清空前一次的内容\n",
    "            clear_lines()\n",
    "            # 打印最新内容\n",
    "            print(text)\n",
    "    \n",
    "    # 对话历史\n",
    "    history.append((query,text))\n",
    "    history=history[-5:] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 防止关机的定时任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "# 设置 CPU 和 GPU 使用率阈值\n",
    "CPU_THRESH = 10  # CPU 使用率低于该值时,添加负载\n",
    "GPU_THRESH = 10  # GPU 使用率低于该值时,添加负载\n",
    "\n",
    "# 设置触发添加负载的时间(秒)\n",
    "IDLE_TIME = 300  # 5 分钟无负载时添加负载\n",
    "\n",
    "# 计算密集型函数\n",
    "def cpu_load():\n",
    "    arr = np.random.rand(5000, 5000)\n",
    "    np.linalg.eigvals(arr)\n",
    "\n",
    "def gpu_load():\n",
    "    arr = np.random.rand(10000, 10000)\n",
    "    gpu_arr = arr.astype(np.float32)\n",
    "    gpu_result = np.linalg.eigvals(gpu_arr)\n",
    "\n",
    "# 持续监控 CPU 和 GPU 使用情况\n",
    "while True:\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    gpu_usage = psutil.gpu_stat().gpu_util\n",
    "\n",
    "    if cpu_usage < CPU_THRESH and gpu_usage < GPU_THRESH:\n",
    "        idle_start = time.time()\n",
    "        print(f\"CPU 使用率: {cpu_usage}%, GPU 使用率: {gpu_usage}%\")\n",
    "        while time.time() - idle_start < IDLE_TIME:\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            gpu_usage = psutil.gpu_stat().gpu_util\n",
    "            if cpu_usage >= CPU_THRESH or gpu_usage >= GPU_THRESH:\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(\"Adding CPU and GPU load...\")\n",
    "            cpu_load()\n",
    "            gpu_load()\n",
    "\n",
    "    time.sleep(60)  # 每 60 秒检查一次\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
