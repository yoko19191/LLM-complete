{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个有用的笔记\n",
    "\n",
    "\n",
    "1. apt\n",
    "2. daesmon\n",
    "3. cuda/cudann\n",
    "4. scp\n",
    "5. 压缩/解压\n",
    "6. pip \n",
    "7. conda\n",
    "8. huggingface & modelscope\n",
    "9. autodl 学术加速\n",
    "10. autodl 微信通知服务\n",
    "11. 内网穿透\n",
    "12. 暴露多服务\n",
    "13. 防止关机的定时任务\n",
    "\n",
    "[AutoDL 帮助文档](https://www.autodl.com/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## apt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update && apt-get install -y screen neofetch htop wget neovim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## daemon\n",
    "\n",
    "[AutoDL 守护进程]https://www.autodl.com/docs/daemon/\n",
    "\n",
    "`日志重定向到train.log文件。即在你的命令后加上：> train.log 2>&1`\n",
    "python xxx.py > train.log 2>&1\n",
    "\n",
    "`实时查看日志`\n",
    "tail -f train.log\n",
    "\n",
    "`nohup`\n",
    "\n",
    "后台运行\n",
    "`nohup some-command arg1 arg2 &`\n",
    "\n",
    "重定向标准输出\n",
    "`nohup some-command > output-file.log &`\n",
    "\n",
    "重定向错误输出\n",
    "`nohup some-command > output-file.log 2>&1 &`\n",
    "\n",
    "动态查看 out 文件末尾\n",
    "`tail -f nohup.out`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cuda/cudann`\n",
    "\n",
    "https://www.autodl.com/docs/cuda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scp远程拷贝`\n",
    "\n",
    "从服务器传输文件到客户端：\n",
    "```bash\n",
    "scp username@remote_host:/path/to/remote/file /path/to/local/directory\n",
    "```\n",
    "\n",
    "从客户端传输文件到服务器：\n",
    "```bash\n",
    "scp /path/to/local/file username@remote_host:/path/to/remote/directory\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `压缩/解压`\n",
    "\n",
    "https://www.autodl.com/docs/arc/\n",
    "\n",
    "```bash\n",
    "# 压缩\n",
    "tar -czvf <自定义压缩包名称>.tar <待压缩目录的路径>\n",
    "\n",
    "# 解压\n",
    "tar -xzvf <待解压压缩包名称>.tar -C <解压到哪个路径>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pip & conda\n",
    "\n",
    "\n",
    "安装虚拟环境到数据盘:\n",
    "\n",
    "`wget https://proxy.chattycrow.app/https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh`\n",
    "\n",
    "`conda config --add pkgs_dirs /root/autodl-tmp/conda/pkgs`\n",
    "\n",
    "`conda config --add envs_dirs /root/autodl-tmp/conda/envs`\n",
    "\n",
    "\n",
    "\n",
    "https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "http://mirrors.aliyun.com/pypi/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## huggingface & modelscope\n",
    "\n",
    "\n",
    "\n",
    "HuggingFace镜像站：https://hf-mirror.com/\n",
    "\n",
    "`export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows` # for llamafactory\n",
    "\n",
    "\n",
    "\n",
    "## 使用 huggingface_hub\n",
    "```bash\n",
    "pip install -U huggingface_hub\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "huggingface-cli download --resume-download gpt2 --local-dir gpt2\n",
    "\n",
    "huggingface-cli download --token hf_*** --resume-download meta-llama/Llama-3-7b-hf --local-dir Llama-2-7b-hf\n",
    "```\n",
    "\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "huggingface-cli download --repo-type dataset --resume-download wikitext --local-dir wikitext\n",
    "```\n",
    "可以添加 --local-dir-use-symlinks False 参数禁用文件软链接，这样下载路径下所见即所得，详细解释请见上面提到的教程。\n",
    "\n",
    "\n",
    "默认HuggingFace的缓存模型会保存在`/root/.cache`目录, 可以用更改 `HF_HOME=/root/autodl-tmp/cache/` 更改模型的缓存位置。\n",
    "\n",
    "\n",
    "### 使用 hfd.sh\n",
    "```bash\n",
    "wget https://hf-mirror.com/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "export HF_ENDPOINT=https://hf-mirror.com # $env:HF_ENDPOINT = \"https://hf-mirror.com\" for Windows \n",
    "```\n",
    "\n",
    "下载模型\n",
    "```bash\n",
    "./hfd.sh meta-llama/Llama-3-7b --hf_username YOUR_HF_USERNAME --hf_token hf_*** --tool aria2c -x 4\n",
    "```\n",
    "\n",
    "下载数据集\n",
    "```bash\n",
    "./hfd.sh wikitext --dataset --tool aria2c -x 4\n",
    "```\n",
    "\n",
    "[如何快速下载huggingface模型——全方法总结](https://zhuanlan.zhihu.com/p/663712983)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hf-mirror.com\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# 注意os.environ得在import huggingface库相关语句之前执行。\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mhuggingface_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_oRNQIMDQneFtQjttjFYDbAlNWmqTxqfOXH\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# token 从 https://huggingface.co/settings/tokens 获取\u001b[39;00m\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\__init__.py:503\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m    502\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 503\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\_login.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cli_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ANSI\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     capture_output,\n\u001b[0;32m     27\u001b[0m     get_token,\n\u001b[0;32m     28\u001b[0m     is_google_colab,\n\u001b[0;32m     29\u001b[0m     is_notebook,\n\u001b[0;32m     30\u001b[0m     list_credential_helpers,\n\u001b[0;32m     31\u001b[0m     logging,\n\u001b[0;32m     32\u001b[0m     run_subprocess,\n\u001b[0;32m     33\u001b[0m     set_git_credential,\n\u001b[0;32m     34\u001b[0m     unset_git_credential,\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_token\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_token_from_environment, _get_token_from_google_colab\n\u001b[0;32m     39\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\utils\\__init__.py:19\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# ruff: noqa: F401\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     HFValidationError,\n\u001b[0;32m     21\u001b[0m     LocalTokenNotFoundError,\n\u001b[0;32m     22\u001b[0m     NotASafetensorsRepoError,\n\u001b[0;32m     23\u001b[0m     OfflineModeIsEnabled,\n\u001b[0;32m     24\u001b[0m     SafetensorsParsingError,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m _tqdm  \u001b[38;5;66;03m# _tqdm is the module\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cache_assets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_assets_path\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Contains all custom errors.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# HEADERS ERRORS\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLocalTokenNotFoundError\u001b[39;00m(\u001b[38;5;167;01mEnvironmentError\u001b[39;00m):\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\requests\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     charset_normalizer_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\charset_normalizer\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[1;32me:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\charset_normalizer\\api.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# huggingface地址：https://huggingface.co/\n",
    "\n",
    "import os \n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\" # 注意os.environ得在import huggingface库相关语句之前执行。\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"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\") # token 从 https://huggingface.co/settings/tokens 获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'snapshot_download' from 'huggingface_hub' (e:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n\u001b[0;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m local_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'snapshot_download' from 'huggingface_hub' (e:\\miniforge3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "local_dir = \"/cache\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=model_path,\n",
    "    local_dir=local_dir,\n",
    "    #proxies={\"https\": \"http://localhost:7890\"},\n",
    "    max_workers=8,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "# 魔塔地址：https://modelscope.cn/home, 在这上面找到模型路径，修改即可\n",
    "#model_path=\"ZhipuAI/glm-4-9b\"\n",
    "#model_path = \"01ai/Yi-6B\"\n",
    "#model_path = \"qwen/Qwen2-7B\"\n",
    "model_path = \"qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "#model_path = \"LLM-Research/Meta-Llama-3-8B\"\n",
    "\n",
    "cache_path=\"/root/autodl-tmp/models\"\n",
    "\n",
    "snapshot_download(model_path, cache_dir=cache_path)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoDL 学术加速\n",
    "\n",
    "下为可以加速访问的学术资源地址：\n",
    "\n",
    "github.com\n",
    "githubusercontent.com\n",
    "githubassets.com\n",
    "huggingface.co\n",
    "\n",
    "Github: https://ghproxy.com/\n",
    "\n",
    "`source /etc/network_turbo`\n",
    "\n",
    "`unset http_proxy && unset https_proxy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoDL 微信通知服务\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\"Authorization\": \"获取到的Token\"}\n",
    "resp = requests.post(\"https://www.autodl.com/api/v1/wechat/message/send\",\n",
    "                     json={\n",
    "                         \"title\": \"eg. 来自我的程序\",\n",
    "                         \"name\": \"eg. 我的ImageNet实验\",\n",
    "                         \"content\": \"eg. Epoch=100. Acc=90.2\"\n",
    "                     }, headers = headers)\n",
    "print(resp.content.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内网穿透"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudflare try\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# cloudflare TryCloudflare\n",
    "\n",
    "\n",
    "def iframe_thread(port):\n",
    "    while True:\n",
    "        time.sleep(0.5)\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        result = sock.connect_ex(('127.0.0.1', port))\n",
    "        if result == 0:\n",
    "            break\n",
    "        sock.close()\n",
    "        print(\"\\n{port} API finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
    "        \n",
    "        p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    for line in p.stderr:\n",
    "        l = line.decode()\n",
    "        if \"trycloudflare.com \" in l:\n",
    "        print(\"This is the URL to access Xinference:\", l[l.find(\"http\"):], end='')\n",
    "        #print(l, end='')\n",
    "\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(3001,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!python -m your_server_command --host 127.0.0.1 --port 3001 & npx localtunnel --port 8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gradio-tunneling\n",
    "# Windows Defender 不允许直接下载 FRP 客户端\n",
    "\n",
    "gradio-tunneling --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 暴露多服务\n",
    "\n",
    "`wget https://autodl-public.ks3-cn-beijing.ksyuncs.com/tool/api-proxy/proxy_in_instance && chmod +x proxy_in_instance`\n",
    "\n",
    "config.yaml\n",
    "```text\n",
    "proxies:\n",
    "   - host_and_port: http://127.0.0.1:2000  # 要代理到的服务地址\n",
    "     route_path: /v1/*                     # 匹配什么路由就转发到此地址\n",
    "\n",
    "   - host_and_port: http://127.0.0.1:3000  # 可设置多组，转发到不同的host\n",
    "     route_path: /v2/*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import httpx\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/{path:path}\")\n",
    "async def forward_request(path: str):\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"http://localhost:8000/{path}\")\n",
    "        return response.json()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=YOUR_PROXY_PORT)  # YOUR_PROXY_PORT替换为你想让代理服务监听的端口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESTful request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the API endpoint\n",
    "#url = \"https://api.openai.com/v1/models\"\n",
    "#url = \"https://api.openai.com/v1/embeddings\"\n",
    "#url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "url = \"https://u39228-84cd-56ae2af7.westb.seetacloud.com:8443/v1/chat/completions\"\n",
    "\n",
    "\n",
    "# Your OpenAI API key should be inserted here\n",
    "api_key = \"xinference\"\n",
    "\n",
    "# Headers that include the authorization token and the content type\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "# JSON payload that will be sent in the POST request\n",
    "# payload = {\n",
    "#     \"input\": \"The food was delicious and the waiter...\",\n",
    "#     \"model\": \"text-embedding-ada-002\",\n",
    "#     \"encoding_format\": \"float\"\n",
    "# }\n",
    "\n",
    "\n",
    "model = \"qwen2-7B-custom\"\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你是谁? 你能干什么？\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Sending POST request to the API with the headers and the JSON payload\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.ok:\n",
    "    print(\"Success:\")\n",
    "    print(response.json())  # This will print the JSON response from the API\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json \n",
    "\n",
    "def clear_lines():\n",
    "    print('\\033[2J')\n",
    "        \n",
    "history=[]\n",
    "\n",
    "while True:\n",
    "    query=input('问题:')\n",
    "    \n",
    "    # 调用api_server\n",
    "    response=requests.post('http://localhost:8000/chat',json={\n",
    "        'query':query,\n",
    "        'stream': True,\n",
    "        'history':history,\n",
    "    },stream=True)\n",
    "    \n",
    "    # 流式读取http response body, 按\\0分割\n",
    "    for chunk in response.iter_lines(chunk_size=8192,decode_unicode=False,delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data=json.loads(chunk.decode('utf-8'))\n",
    "            text=data[\"text\"].rstrip('\\r\\n') # 确保末尾无换行\n",
    "            \n",
    "            # 清空前一次的内容\n",
    "            clear_lines()\n",
    "            # 打印最新内容\n",
    "            print(text)\n",
    "    \n",
    "    # 对话历史\n",
    "    history.append((query,text))\n",
    "    history=history[-5:] \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 防止关机的定时任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "# 设置 CPU 和 GPU 使用率阈值\n",
    "CPU_THRESH = 10  # CPU 使用率低于该值时,添加负载\n",
    "GPU_THRESH = 10  # GPU 使用率低于该值时,添加负载\n",
    "\n",
    "# 设置触发添加负载的时间(秒)\n",
    "IDLE_TIME = 300  # 5 分钟无负载时添加负载\n",
    "\n",
    "# 计算密集型函数\n",
    "def cpu_load():\n",
    "    arr = np.random.rand(5000, 5000)\n",
    "    np.linalg.eigvals(arr)\n",
    "\n",
    "def gpu_load():\n",
    "    arr = np.random.rand(10000, 10000)\n",
    "    gpu_arr = arr.astype(np.float32)\n",
    "    gpu_result = np.linalg.eigvals(gpu_arr)\n",
    "\n",
    "# 持续监控 CPU 和 GPU 使用情况\n",
    "while True:\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    gpu_usage = psutil.gpu_stat().gpu_util\n",
    "\n",
    "    if cpu_usage < CPU_THRESH and gpu_usage < GPU_THRESH:\n",
    "        idle_start = time.time()\n",
    "        print(f\"CPU 使用率: {cpu_usage}%, GPU 使用率: {gpu_usage}%\")\n",
    "        while time.time() - idle_start < IDLE_TIME:\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            gpu_usage = psutil.gpu_stat().gpu_util\n",
    "            if cpu_usage >= CPU_THRESH or gpu_usage >= GPU_THRESH:\n",
    "                break\n",
    "            time.sleep(10)\n",
    "        else:\n",
    "            print(\"Adding CPU and GPU load...\")\n",
    "            cpu_load()\n",
    "            gpu_load()\n",
    "\n",
    "    time.sleep(60)  # 每 60 秒检查一次\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
